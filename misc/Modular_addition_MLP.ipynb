{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-a3SvAWo_Bi9"
      },
      "source": [
        "# Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ber1e1hYAjkE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "np.random.seed(1337)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69nGoOmgLChf"
      },
      "source": [
        "# Model interpretation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn9UV7ZdB_AU"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_tPBjVR16ig",
        "outputId": "51b98315-0cbe-4611-b31b-e6b85e6e027b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2000\n",
            "Epoch: 0 | Train loss: 4.75 | Validation loss: 4.74 | Train accuracy: 0.00 | Validation accuracy: 0.00\n",
            "Epoch: 1 | Train loss: 4.74 | Validation loss: 4.75 | Train accuracy: 0.02 | Validation accuracy: 0.00\n",
            "Epoch: 2 | Train loss: 4.73 | Validation loss: 4.76 | Train accuracy: 0.02 | Validation accuracy: 0.00\n",
            "Epoch: 3 | Train loss: 4.72 | Validation loss: 4.77 | Train accuracy: 0.02 | Validation accuracy: 0.00\n",
            "Epoch: 4 | Train loss: 4.67 | Validation loss: 4.74 | Train accuracy: 0.04 | Validation accuracy: 0.00\n",
            "Epoch: 5 | Train loss: 4.55 | Validation loss: 4.63 | Train accuracy: 0.10 | Validation accuracy: 0.02\n",
            "Epoch: 6 | Train loss: 4.30 | Validation loss: 4.37 | Train accuracy: 0.21 | Validation accuracy: 0.07\n",
            "Epoch: 7 | Train loss: 3.88 | Validation loss: 3.96 | Train accuracy: 0.37 | Validation accuracy: 0.19\n",
            "Epoch: 8 | Train loss: 3.32 | Validation loss: 3.43 | Train accuracy: 0.55 | Validation accuracy: 0.34\n",
            "Epoch: 9 | Train loss: 2.70 | Validation loss: 2.88 | Train accuracy: 0.71 | Validation accuracy: 0.51\n",
            "Epoch: 10 | Train loss: 2.13 | Validation loss: 2.39 | Train accuracy: 0.83 | Validation accuracy: 0.66\n",
            "Epoch: 11 | Train loss: 1.69 | Validation loss: 2.02 | Train accuracy: 0.90 | Validation accuracy: 0.75\n",
            "Epoch: 12 | Train loss: 1.39 | Validation loss: 1.75 | Train accuracy: 0.94 | Validation accuracy: 0.83\n",
            "Epoch: 13 | Train loss: 1.20 | Validation loss: 1.57 | Train accuracy: 0.96 | Validation accuracy: 0.87\n",
            "Epoch: 14 | Train loss: 1.07 | Validation loss: 1.44 | Train accuracy: 0.97 | Validation accuracy: 0.90\n",
            "Epoch: 15 | Train loss: 0.98 | Validation loss: 1.34 | Train accuracy: 0.98 | Validation accuracy: 0.92\n",
            "Epoch: 16 | Train loss: 0.91 | Validation loss: 1.27 | Train accuracy: 0.99 | Validation accuracy: 0.94\n",
            "Epoch: 17 | Train loss: 0.86 | Validation loss: 1.21 | Train accuracy: 0.99 | Validation accuracy: 0.95\n",
            "Epoch: 18 | Train loss: 0.83 | Validation loss: 1.17 | Train accuracy: 0.99 | Validation accuracy: 0.95\n",
            "Epoch: 19 | Train loss: 0.80 | Validation loss: 1.13 | Train accuracy: 0.99 | Validation accuracy: 0.96\n",
            "Epoch: 20 | Train loss: 0.78 | Validation loss: 1.10 | Train accuracy: 0.99 | Validation accuracy: 0.97\n",
            "Epoch: 21 | Train loss: 0.76 | Validation loss: 1.07 | Train accuracy: 1.00 | Validation accuracy: 0.97\n",
            "Epoch: 22 | Train loss: 0.74 | Validation loss: 1.05 | Train accuracy: 1.00 | Validation accuracy: 0.97\n",
            "Epoch: 23 | Train loss: 0.72 | Validation loss: 1.03 | Train accuracy: 1.00 | Validation accuracy: 0.97\n",
            "Epoch: 24 | Train loss: 0.71 | Validation loss: 1.01 | Train accuracy: 1.00 | Validation accuracy: 0.98\n",
            "Epoch: 25 | Train loss: 0.70 | Validation loss: 0.99 | Train accuracy: 1.00 | Validation accuracy: 0.98\n",
            "Epoch: 26 | Train loss: 0.69 | Validation loss: 0.98 | Train accuracy: 1.00 | Validation accuracy: 0.98\n",
            "Epoch: 27 | Train loss: 0.68 | Validation loss: 0.96 | Train accuracy: 1.00 | Validation accuracy: 0.98\n",
            "Epoch: 28 | Train loss: 0.67 | Validation loss: 0.95 | Train accuracy: 1.00 | Validation accuracy: 0.98\n",
            "Epoch: 29 | Train loss: 0.66 | Validation loss: 0.94 | Train accuracy: 1.00 | Validation accuracy: 0.99\n",
            "Epoch: 30 | Train loss: 0.65 | Validation loss: 0.93 | Train accuracy: 1.00 | Validation accuracy: 0.99\n",
            "Epoch: 31 | Train loss: 0.64 | Validation loss: 0.92 | Train accuracy: 1.00 | Validation accuracy: 0.99\n",
            "Epoch: 32 | Train loss: 0.63 | Validation loss: 0.91 | Train accuracy: 1.00 | Validation accuracy: 0.99\n",
            "Epoch: 33 | Train loss: 0.63 | Validation loss: 0.90 | Train accuracy: 1.00 | Validation accuracy: 0.99\n",
            "Epoch: 34 | Train loss: 0.62 | Validation loss: 0.89 | Train accuracy: 1.00 | Validation accuracy: 0.99\n",
            "Epoch: 35 | Train loss: 0.62 | Validation loss: 0.88 | Train accuracy: 1.00 | Validation accuracy: 0.99\n",
            "Epoch: 36 | Train loss: 0.61 | Validation loss: 0.87 | Train accuracy: 1.00 | Validation accuracy: 0.99\n",
            "Epoch: 37 | Train loss: 0.60 | Validation loss: 0.87 | Train accuracy: 1.00 | Validation accuracy: 0.99\n",
            "Epoch: 38 | Train loss: 0.60 | Validation loss: 0.86 | Train accuracy: 1.00 | Validation accuracy: 0.99\n",
            "Epoch: 39 | Train loss: 0.59 | Validation loss: 0.85 | Train accuracy: 1.00 | Validation accuracy: 0.99\n",
            "Epoch: 40 | Train loss: 0.59 | Validation loss: 0.84 | Train accuracy: 1.00 | Validation accuracy: 0.99\n",
            "Epoch: 41 | Train loss: 0.58 | Validation loss: 0.84 | Train accuracy: 1.00 | Validation accuracy: 0.99\n",
            "Epoch: 42 | Train loss: 0.58 | Validation loss: 0.83 | Train accuracy: 1.00 | Validation accuracy: 0.99\n",
            "Epoch: 43 | Train loss: 0.57 | Validation loss: 0.83 | Train accuracy: 1.00 | Validation accuracy: 0.99\n",
            "Epoch: 44 | Train loss: 0.57 | Validation loss: 0.82 | Train accuracy: 1.00 | Validation accuracy: 0.99\n",
            "Epoch: 45 | Train loss: 0.56 | Validation loss: 0.81 | Train accuracy: 1.00 | Validation accuracy: 0.99\n",
            "Epoch: 46 | Train loss: 0.56 | Validation loss: 0.81 | Train accuracy: 1.00 | Validation accuracy: 0.99\n",
            "Epoch: 47 | Train loss: 0.56 | Validation loss: 0.80 | Train accuracy: 1.00 | Validation accuracy: 0.99\n",
            "Epoch: 48 | Train loss: 0.55 | Validation loss: 0.80 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 49 | Train loss: 0.55 | Validation loss: 0.79 | Train accuracy: 1.00 | Validation accuracy: 0.99\n",
            "Epoch: 50 | Train loss: 0.54 | Validation loss: 0.79 | Train accuracy: 1.00 | Validation accuracy: 0.99\n",
            "Epoch: 51 | Train loss: 0.54 | Validation loss: 0.78 | Train accuracy: 1.00 | Validation accuracy: 0.99\n",
            "Epoch: 52 | Train loss: 0.54 | Validation loss: 0.78 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 53 | Train loss: 0.53 | Validation loss: 0.77 | Train accuracy: 1.00 | Validation accuracy: 0.99\n",
            "Epoch: 54 | Train loss: 0.53 | Validation loss: 0.77 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 55 | Train loss: 0.53 | Validation loss: 0.77 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 56 | Train loss: 0.52 | Validation loss: 0.76 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 57 | Train loss: 0.52 | Validation loss: 0.76 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 58 | Train loss: 0.52 | Validation loss: 0.75 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 59 | Train loss: 0.51 | Validation loss: 0.75 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 60 | Train loss: 0.51 | Validation loss: 0.75 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 61 | Train loss: 0.51 | Validation loss: 0.74 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 62 | Train loss: 0.51 | Validation loss: 0.74 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 63 | Train loss: 0.50 | Validation loss: 0.74 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 64 | Train loss: 0.50 | Validation loss: 0.73 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 65 | Train loss: 0.50 | Validation loss: 0.73 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 66 | Train loss: 0.50 | Validation loss: 0.73 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 67 | Train loss: 0.49 | Validation loss: 0.73 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 68 | Train loss: 0.49 | Validation loss: 0.72 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 69 | Train loss: 0.49 | Validation loss: 0.72 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 70 | Train loss: 0.49 | Validation loss: 0.72 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 71 | Train loss: 0.49 | Validation loss: 0.72 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 72 | Train loss: 0.48 | Validation loss: 0.71 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 73 | Train loss: 0.48 | Validation loss: 0.71 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 74 | Train loss: 0.48 | Validation loss: 0.71 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 75 | Train loss: 0.48 | Validation loss: 0.71 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 76 | Train loss: 0.48 | Validation loss: 0.70 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 77 | Train loss: 0.47 | Validation loss: 0.70 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 78 | Train loss: 0.47 | Validation loss: 0.70 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 79 | Train loss: 0.47 | Validation loss: 0.70 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 80 | Train loss: 0.47 | Validation loss: 0.69 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 81 | Train loss: 0.47 | Validation loss: 0.69 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 82 | Train loss: 0.46 | Validation loss: 0.69 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 83 | Train loss: 0.46 | Validation loss: 0.69 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 84 | Train loss: 0.46 | Validation loss: 0.69 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 85 | Train loss: 0.46 | Validation loss: 0.68 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 86 | Train loss: 0.46 | Validation loss: 0.68 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 87 | Train loss: 0.46 | Validation loss: 0.68 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 88 | Train loss: 0.46 | Validation loss: 0.68 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 89 | Train loss: 0.45 | Validation loss: 0.68 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 90 | Train loss: 0.45 | Validation loss: 0.68 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 91 | Train loss: 0.45 | Validation loss: 0.67 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 92 | Train loss: 0.45 | Validation loss: 0.67 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 93 | Train loss: 0.45 | Validation loss: 0.67 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 94 | Train loss: 0.45 | Validation loss: 0.67 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 95 | Train loss: 0.45 | Validation loss: 0.67 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 96 | Train loss: 0.45 | Validation loss: 0.67 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 97 | Train loss: 0.44 | Validation loss: 0.67 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 98 | Train loss: 0.44 | Validation loss: 0.66 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 99 | Train loss: 0.44 | Validation loss: 0.66 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 100 | Train loss: 0.44 | Validation loss: 0.66 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 101 | Train loss: 0.44 | Validation loss: 0.66 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 102 | Train loss: 0.44 | Validation loss: 0.66 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 103 | Train loss: 0.44 | Validation loss: 0.66 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 104 | Train loss: 0.44 | Validation loss: 0.66 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 105 | Train loss: 0.44 | Validation loss: 0.66 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 106 | Train loss: 0.43 | Validation loss: 0.65 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 107 | Train loss: 0.43 | Validation loss: 0.65 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 108 | Train loss: 0.43 | Validation loss: 0.65 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 109 | Train loss: 0.43 | Validation loss: 0.65 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 110 | Train loss: 0.43 | Validation loss: 0.65 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 111 | Train loss: 0.43 | Validation loss: 0.65 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 112 | Train loss: 0.43 | Validation loss: 0.65 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 113 | Train loss: 0.43 | Validation loss: 0.65 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 114 | Train loss: 0.43 | Validation loss: 0.64 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 115 | Train loss: 0.43 | Validation loss: 0.64 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 116 | Train loss: 0.43 | Validation loss: 0.64 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 117 | Train loss: 0.42 | Validation loss: 0.64 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 118 | Train loss: 0.42 | Validation loss: 0.64 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 119 | Train loss: 0.42 | Validation loss: 0.64 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 120 | Train loss: 0.42 | Validation loss: 0.64 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 121 | Train loss: 0.42 | Validation loss: 0.64 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 122 | Train loss: 0.42 | Validation loss: 0.64 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 123 | Train loss: 0.42 | Validation loss: 0.64 | Train accuracy: 1.00 | Validation accuracy: 1.00\n",
            "Epoch: 124 | Train loss: 0.42 | Validation loss: 0.64 | Train accuracy: 1.00 | Validation accuracy: 1.00\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 213\u001b[0m\n\u001b[0;32m    211\u001b[0m model \u001b[38;5;241m=\u001b[39m MLP(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mp, \u001b[38;5;241m100\u001b[39m, p)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    212\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m\n\u001b[1;32m--> 213\u001b[0m model, train_loss_values, val_loss_values, train_acc_values, val_acc_values \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.003\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m model\u001b[38;5;241m.\u001b[39mcalculate_neuron_properties()\n",
            "Cell \u001b[1;32mIn[4], line 170\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_data, train_labels, val_data, val_labels, epochs, batch_size, lr)\u001b[0m\n\u001b[0;32m    168\u001b[0m optimzer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    169\u001b[0m output, pre_relu, post_relu \u001b[38;5;241m=\u001b[39m model(train_data[batch:batch\u001b[38;5;241m+\u001b[39mbatch_size])\n\u001b[1;32m--> 170\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m#+ 0.0000001*l1_norm\u001b[39;00m\n\u001b[0;32m    171\u001b[0m running_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    172\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(output, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Check if CUDA is available, else use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# define the dataset for modular addition\n",
        "p = 113\n",
        "data_set = np.zeros((p*p, 2*p))\n",
        "labels = np.zeros((p*p, p))\n",
        "for number1 in range(p):\n",
        "  for number2 in range(p):\n",
        "    data_set[number1*p + number2][number1] = 1\n",
        "    data_set[number1*p + number2][number2+p] = 1\n",
        "    labels[number1*p + number2][(number1 + number2) % p] = 1\n",
        "\n",
        "# shuffle the dataset\n",
        "shuffle = np.random.permutation(p*p)\n",
        "data_set = data_set[shuffle]\n",
        "labels = labels[shuffle]\n",
        "\n",
        "# divide in train and validation set\n",
        "train_proportion = 0.8\n",
        "train_data = data_set[:int(train_proportion*p*p)]\n",
        "train_labels = labels[:int(train_proportion*p*p)]\n",
        "val_data = data_set[int(train_proportion*p*p):]\n",
        "val_labels = labels[int(train_proportion*p*p):]\n",
        "\n",
        "# convert to tensors\n",
        "train_data = torch.from_numpy(train_data).float().to(device)\n",
        "train_labels = torch.from_numpy(train_labels).float().to(device)\n",
        "val_data = torch.from_numpy(val_data).float().to(device)\n",
        "val_labels = torch.from_numpy(val_labels).float().to(device)\n",
        "\n",
        "\n",
        "# define the 1-hidden layer MLP\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(input_size, hidden_size, bias=False)\n",
        "        self.fc2 = torch.nn.Linear(hidden_size, output_size, bias=False)\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.u1 = torch.zeros(hidden_size).to(device)\n",
        "        self.u2 = torch.zeros(hidden_size).to(device)\n",
        "        self.u3 = torch.zeros(hidden_size).to(device)\n",
        "        self.w1 = torch.zeros(hidden_size).to(device)\n",
        "        self.w2 = torch.zeros(hidden_size).to(device)\n",
        "        self.w3 = torch.zeros(hidden_size).to(device)\n",
        "        self.s1 = torch.zeros(hidden_size).to(device)\n",
        "        self.s2 = torch.zeros(hidden_size).to(device)\n",
        "        self.s3 = torch.zeros(hidden_size).to(device)\n",
        "        self.o1 = torch.zeros(hidden_size).to(device)\n",
        "        self.o2 = torch.zeros(hidden_size).to(device)\n",
        "        self.o3 = torch.zeros(hidden_size).to(device)\n",
        "\n",
        "\n",
        "    def forward(self, x, val=False):\n",
        "        pre_relu = self.fc1(x)\n",
        "        post_relu = torch.nn.functional.relu(pre_relu)\n",
        "        x = self.fc2(post_relu)\n",
        "        return x, pre_relu, post_relu\n",
        "\n",
        "\n",
        "    def cosine(self, x, u, w, s, o):\n",
        "      return u*torch.cos(x*w*2*np.pi + s) + o\n",
        "\n",
        "\n",
        "    def forward_with_replaced_weights(self, x, p_l1=0, plot=True, altered_i=None):\n",
        "        # Cloning the weights before modification\n",
        "        fc1_weight_original = self.fc1.weight.data.clone()\n",
        "        fc2_weight_original = self.fc2.weight.data.clone()\n",
        "\n",
        "        a = torch.argmax(x[:, :113], dim=1).unsqueeze(-1)\n",
        "        b = torch.argmax(x[:, 113:], dim=1).unsqueeze(-1)\n",
        "\n",
        "        pre_relu = self.fc1(x)\n",
        "        if plot:\n",
        "            plt.matshow(pre_relu[:, 0].cpu().detach().numpy().reshape(113, 113))\n",
        "            plt.show()\n",
        "\n",
        "        if not altered_i:\n",
        "          num_altered_neurons = int(p_l1 * pre_relu.shape[-1])\n",
        "          altered_i = torch.randperm(pre_relu.shape[-1])[:num_altered_neurons].to(device)\n",
        "\n",
        "\n",
        "        for i in altered_i:\n",
        "            # Replace incoming weights of the altered neurons\n",
        "            self.fc1.weight.data[i, :113] = self.cosine(torch.arange(113).to(device), self.u1[i], self.w1[i], self.s1[i], self.o1[i])\n",
        "            self.fc1.weight.data[i, 113:] = self.cosine(torch.arange(113).to(device), self.u2[i], self.w2[i], self.s2[i], self.o2[i])\n",
        "\n",
        "            # Replace outgoing weights of the altered neurons\n",
        "            self.fc2.weight.data[:, i] = self.cosine(torch.arange(113).to(device), self.u3[i], self.w3[i], self.s3[i], self.o3[i])\n",
        "\n",
        "        pre_relu = self.fc1(x)  # Re-compute pre_relu with updated weights\n",
        "\n",
        "        if plot:\n",
        "            plt.matshow(pre_relu[:, 0].cpu().detach().numpy().reshape(113, 113))\n",
        "            plt.show()\n",
        "\n",
        "        post_relu = torch.nn.functional.relu(pre_relu)\n",
        "        output = self.fc2(post_relu)\n",
        "\n",
        "        # Restoring the original weights after forward pass\n",
        "        self.fc1.weight.data = fc1_weight_original\n",
        "        self.fc2.weight.data = fc2_weight_original\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "    def calculate_neuron_properties(self):\n",
        "        for neuron in range(self.hidden_size):\n",
        "          weights1 = self.fc1.weight.detach().cpu().numpy()[neuron, :113]\n",
        "          weights2 = self.fc1.weight.detach().cpu().numpy()[neuron, 113:]\n",
        "          weights3 = self.fc2.weight.detach().cpu().numpy()[:, neuron]\n",
        "          x_data = np.arange(self.output_size)\n",
        "          w1, s1, u1, o1 = self.find_cosine(x_data, weights1)\n",
        "          w2, s2, u2, o2 = self.find_cosine(x_data, weights2)\n",
        "          w3, s3, u3, o3 = self.find_cosine(x_data, weights3)\n",
        "          self.u1[neuron] = u1\n",
        "          self.u2[neuron] = u2\n",
        "          self.u3[neuron] = u3\n",
        "          self.w1[neuron] = w1\n",
        "          self.w2[neuron] = w2\n",
        "          self.w3[neuron] = w3\n",
        "          self.s1[neuron] = s1\n",
        "          self.s2[neuron] = s2\n",
        "          self.s3[neuron] = s3\n",
        "          self.o1[neuron] = torch.tensor(o1)\n",
        "          self.o2[neuron] = torch.tensor(o2)\n",
        "          self.o3[neuron] = torch.tensor(o3)\n",
        "\n",
        "\n",
        "    def find_cosine(self, x_data, y_data):\n",
        "        # Calculate DFT\n",
        "        yf = np.fft.fft(y_data)\n",
        "        xf = np.fft.fftfreq(x_data.size, d=(x_data[1]-x_data[0]))  # assuming x_data is evenly spaced\n",
        "\n",
        "        # Find the peak frequency\n",
        "        idx = np.argmax(np.abs(yf[1:yf.size//2]))  # ignore the zero frequency \"peak\", and only consider the first half of points\n",
        "        freq = np.abs(xf[idx+1])  # shift index by 1 because we ignored the first point\n",
        "\n",
        "        # Calculate phase shift\n",
        "        phase_shift = -np.angle(yf[idx+1])\n",
        "\n",
        "        # Calculate scale\n",
        "        scale = 2 * np.abs(yf[idx+1]) / x_data.size\n",
        "\n",
        "        # Estimate offset\n",
        "        offset = np.mean(y_data)\n",
        "        return freq, phase_shift, scale, offset\n",
        "\n",
        "\n",
        "# define the training loop\n",
        "def train(model, train_data, train_labels, val_data, val_labels, epochs, batch_size, lr):\n",
        "    optimzer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.5)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    train_loss_values = []\n",
        "    val_loss_values = []\n",
        "    train_acc_values = []\n",
        "    val_acc_values = []\n",
        "\n",
        "    running_train_loss = 0\n",
        "    print(epochs)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        correct_train_preds = 0\n",
        "        total_train_preds = 0\n",
        "        for batch in range(0, len(train_data), batch_size):\n",
        "            optimzer.zero_grad()\n",
        "            output, pre_relu, post_relu = model(train_data[batch:batch+batch_size])\n",
        "            loss = loss_fn(output, torch.argmax(train_labels[batch:batch+batch_size], axis=1)) #+ 0.0000001*l1_norm\n",
        "            running_train_loss += loss.item()\n",
        "            preds = torch.argmax(output, axis=1)\n",
        "            correct_train_preds += (preds == torch.argmax(train_labels[batch:batch+batch_size], axis=1)).sum().item()\n",
        "            total_train_preds += len(preds)\n",
        "            loss.backward()\n",
        "            optimzer.step()\n",
        "        model.eval()\n",
        "\n",
        "\n",
        "        output, _, _ = model(val_data, val=True)\n",
        "        val_loss = loss_fn(output, torch.argmax(val_labels, axis=1)).item()\n",
        "        val_preds = torch.argmax(output, axis=1)\n",
        "        correct_val_preds = (val_preds == torch.argmax(val_labels, axis=1)).sum().item()\n",
        "        total_val_preds = len(val_preds)\n",
        "        avg_train_loss = running_train_loss / (len(train_data) / batch_size)\n",
        "        train_acc = correct_train_preds / total_train_preds\n",
        "        val_acc = correct_val_preds / total_val_preds\n",
        "        train_loss_values.append(avg_train_loss)\n",
        "        val_loss_values.append(val_loss)\n",
        "        train_acc_values.append(train_acc)\n",
        "        val_acc_values.append(val_acc)\n",
        "\n",
        "        print(\"Epoch: {} | Train loss: {:.2f} | Validation loss: {:.2f} | Train accuracy: {:.2f} | Validation accuracy: {:.2f}\".format(epoch, avg_train_loss, val_loss, train_acc, val_acc))\n",
        "\n",
        "        running_train_loss = 0\n",
        "    return model, train_loss_values, val_loss_values, train_acc_values, val_acc_values\n",
        "\n",
        "    def forward_with_replaced_neurons(self, x, p_l1=0, p_l2=0, plot=True):\n",
        "        a = torch.argmax(x[:, :113], dim=1).unsqueeze(-1)\n",
        "        b = torch.argmax(x[:, 113:], dim=1).unsqueeze(-1)\n",
        "\n",
        "        output[:, output_neuron_i] = 0\n",
        "\n",
        "        for hidden_neuron_i in range(self.hidden_size):\n",
        "                output[:, output_neuron_i] += post_relu[:, hidden_neuron_i] * \\\n",
        "                                              self.cosine(output_neuron_i, self.u3[hidden_neuron_i], self.w3[hidden_neuron_i], self.s3[hidden_neuron_i], self.o3[hidden_neuron_i])\n",
        "\n",
        "        return output\n",
        "\n",
        "# train the model\n",
        "model = MLP(2*p, 100, p).to(device)\n",
        "num_epochs = 2000\n",
        "model, train_loss_values, val_loss_values, train_acc_values, val_acc_values = train(model, train_data, train_labels, val_data, val_labels, num_epochs, 128, 0.003)\n",
        "model.calculate_neuron_properties()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xT66RXRDkCJ"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2, 1, figsize=(5, 5))  # create a new figure with 2 subplots arranged vertically\n",
        "\n",
        "# Plotting the loss values\n",
        "ax[0].semilogy(np.arange(num_epochs), train_loss_values, label='Training Loss')\n",
        "ax[0].semilogy(np.arange(num_epochs), val_loss_values, label='Validation Loss')\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_title('Training and Validation Loss')\n",
        "ax[0].legend(loc='upper right')\n",
        "ax[0].grid(True)\n",
        "ax[0].set_xscale('log')  # Making the x-axis logarithmic\n",
        "\n",
        "# Plotting the accuracy values\n",
        "ax[1].plot(np.arange(num_epochs), train_acc_values, label='Training Accuracy')\n",
        "ax[1].plot(np.arange(num_epochs), val_acc_values, label='Validation Accuracy')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Accuracy')\n",
        "ax[1].set_title('Training and Validation Accuracy')\n",
        "ax[1].legend(loc='lower right')\n",
        "ax[1].grid(True)\n",
        "ax[1].set_xscale('log')  # Making the x-axis logarithmic\n",
        "\n",
        "plt.tight_layout()  # To ensure proper spacing between subplots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agptxPQDHWzQ"
      },
      "source": [
        "## Inspecting the Neurons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UdGAE0rQ2t1"
      },
      "outputs": [],
      "source": [
        "# generate the new data\n",
        "new_data = np.zeros((p*p, 2*p))\n",
        "for number1 in range(p):\n",
        "  for number2 in range(p):\n",
        "    new_data[number1*p + number2][number1] = 1\n",
        "    new_data[number1*p + number2][number2+p] = 1\n",
        "\n",
        "# convert to tensor and move to GPU\n",
        "new_data_tensor = torch.from_numpy(new_data).float().to(device)\n",
        "\n",
        "# pass the data through the model and get the post-relu activations\n",
        "model.eval()\n",
        "with torch.no_grad():# generate the new data\n",
        "    _, pre_relu_activations, post_relu_activations = model(new_data_tensor)\n",
        "\n",
        "\n",
        "# Select four random neuron indices\n",
        "random_neurons = np.random.choice(100, 4, replace=False)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
        "\n",
        "for i, neuron in enumerate(random_neurons):\n",
        "  # get the activations of the neuron\n",
        "  neuron_activations = post_relu_activations[:, neuron].cpu().numpy()\n",
        "\n",
        "  # reshape the activations into a 2D array for plotting\n",
        "  neuron_activations = neuron_activations.reshape(p, p)\n",
        "\n",
        "  # plot the activations as an image\n",
        "  ax = axes[i // 2, i % 2]\n",
        "  im = ax.imshow(neuron_activations, cmap='hot', interpolation='nearest')\n",
        "\n",
        "# Add a colorbar to the figure, for all subplots\n",
        "fig.colorbar(im, ax=axes.ravel().tolist(), orientation='horizontal')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN3F2_hFw9zZ"
      },
      "outputs": [],
      "source": [
        "def find_cosine(x_data, y_data):\n",
        "  # Calculate DFT\n",
        "  yf = np.fft.fft(y_data)\n",
        "  xf = np.fft.fftfreq(x_data.size, d=(x_data[1]-x_data[0]))  # assuming x_data is evenly spaced\n",
        "\n",
        "  # Find the peak frequency\n",
        "  idx = np.argmax(np.abs(yf[1:yf.size//2]))  # ignore the zero frequency \"peak\", and only consider the first half of points\n",
        "  freq = xf[idx+1]  # shift index by 1 because we ignored the first point\n",
        "\n",
        "  # Calculate phase shift\n",
        "  phase_shift = np.angle(yf[idx+1])\n",
        "\n",
        "  # Calculate scale\n",
        "  scale = 2 * np.abs(yf[idx+1]) / x_data.size\n",
        "\n",
        "  # Estimate offset\n",
        "  offset = np.mean(y_data)\n",
        "\n",
        "  return scale, freq, phase_shift, offset\n",
        "\n",
        "weights1 = model.fc1.weight.detach().cpu().numpy()\n",
        "x_data = np.arange(113)  # fill this in with your x data\n",
        "\n",
        "# Initialize lists to store parameters\n",
        "scales, frequencies, phase_shifts, offsets = [], [], [], []\n",
        "scales2, frequencies2, phase_shifts2, offsets2 = [], [], [], []\n",
        "\n",
        "# Loop through all neurons\n",
        "for neuron in range(100):\n",
        "    y_data = weights1[neuron, :113]  # fill this in with your y data\n",
        "    scale, frequency, phase_shift, offset = find_cosine(x_data, y_data)\n",
        "    scales.append(scale)\n",
        "    frequencies.append(frequency)\n",
        "    phase_shifts.append(phase_shift)\n",
        "    offsets.append(offset)\n",
        "\n",
        "    y_data = weights1[neuron, 113:]  # fill this in with your y data\n",
        "    scale2, frequency2, phase_shift2, offset2 = find_cosine(x_data, y_data)\n",
        "    scales2.append(scale2)\n",
        "    frequencies2.append(frequency2)\n",
        "    phase_shifts2.append(phase_shift2)\n",
        "    offsets2.append(offset2)\n",
        "\n",
        "# Create scatter plots\n",
        "fig, axs = plt.subplots(2, 2, figsize=(10, 5))\n",
        "\n",
        "params = [(scales, scales2), (frequencies, frequencies2),\n",
        "          (phase_shifts, phase_shifts2), (offsets, offsets2)]\n",
        "titles = ['Scale', 'Frequency', 'Phase Shift', 'Offset']\n",
        "\n",
        "for ax, param, title in zip(axs.flatten(), params, titles):\n",
        "    min_val = min(min(param[0]), min(param[1]))\n",
        "    max_val = max(max(param[0]), max(param[1]))\n",
        "    ax.set_xlim([min_val, max_val])\n",
        "    ax.set_ylim([min_val, max_val])\n",
        "    ax.scatter(param[0], param[1], alpha=0.7, edgecolors='w', s=60)\n",
        "    ax.plot([min_val, max_val], [min_val, max_val], 'k--')  # line X = Y\n",
        "    ax.set_title(title, fontsize=16)\n",
        "    ax.set_xlabel('Weights from a', fontsize=10)\n",
        "    ax.set_ylabel('Weights from b', fontsize=10)\n",
        "\n",
        "# Add overarching title\n",
        "fig.suptitle('Comparison of Cosine Parameters for all Neurons', fontsize=20)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CgPWZVyK3Wq"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxjQkeJ41roW"
      },
      "source": [
        "#Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ld-RQ3Pyub9"
      },
      "outputs": [],
      "source": [
        "def calculate_phase(A, B, s1, s2, f):\n",
        "  result = []\n",
        "  for i in range(len(s1)):\n",
        "    if np.abs(s1[i] + s2[i]) < np.pi:\n",
        "      result.append(s1[i] + s2[i])\n",
        "    elif s1[i] + s2[i] < -1*np.pi:\n",
        "      result.append(s1[i] + s2[i]+ 2*np.pi)\n",
        "    elif s1[i] + s2[i] > np.pi:\n",
        "      result.append(s1[i] + s2[i] - 2*np.pi)\n",
        "\n",
        "  return np.asarray(result)\n",
        "\n",
        "weights1 = model.fc1.weight.detach().cpu().numpy()\n",
        "weights2 = model.fc2.weight.detach().cpu().numpy()\n",
        "x_data = np.arange(113)  # fill this in with your x data\n",
        "\n",
        "# Initialize lists to store parameters\n",
        "scales, frequencies, phase_shifts, offsets = [], [], [], []\n",
        "scales2, frequencies2, phase_shifts2, offsets2 = [], [], [], []\n",
        "scales3, frequencies3, phase_shifts3, offsets3 = [], [], [], []\n",
        "\n",
        "\n",
        "# Loop through all neurons\n",
        "for neuron in range(100):\n",
        "    y_data = weights1[neuron, :113]  # fill this in with your y data\n",
        "    scale, frequency, phase_shift, offset = find_cosine(x_data, y_data)\n",
        "    scales.append(scale)\n",
        "    frequencies.append(frequency)\n",
        "    phase_shifts.append(phase_shift)\n",
        "    offsets.append(offset)\n",
        "\n",
        "    y_data = weights1[neuron, 113:]  # fill this in with your y data\n",
        "    scale2, frequency2, phase_shift2, offset2 = find_cosine(x_data, y_data)\n",
        "    scales2.append(scale2)\n",
        "    frequencies2.append(frequency2)\n",
        "    phase_shifts2.append(phase_shift2)\n",
        "    offsets2.append(offset2)\n",
        "\n",
        "    y_data = weights2[:, neuron]  # fill this in with your y data\n",
        "    scale3, frequency3, phase_shift3, offset3 = find_cosine(x_data, y_data)\n",
        "    scales3.append(scale3)\n",
        "    frequencies3.append(frequency2)\n",
        "    phase_shifts3.append(phase_shift3)\n",
        "    offsets3.append(offset3)\n",
        "\n",
        "\n",
        "# Create scatter plots\n",
        "fig, axs = plt.subplots(2, 2, figsize=(10, 5))\n",
        "\n",
        "params = [(np.array(scales), scales3), (frequencies, frequencies3),\n",
        "          (calculate_phase(np.asarray(scales), np.asarray(scales2), np.asarray(phase_shifts), np.asarray(phase_shifts2), np.asarray(frequencies)), phase_shifts3), (offsets, offsets3)]\n",
        "titles = ['Scale', 'Frequency', 'Phase Shift', 'Offset']\n",
        "\n",
        "for ax, param, title in zip(axs.flatten(), params, titles):\n",
        "    min_val = min(min(param[0]), min(param[1]))\n",
        "    max_val = max(max(param[0]), max(param[1]))\n",
        "    ax.set_xlim([min_val, max_val])\n",
        "    ax.set_ylim([min_val, max_val])\n",
        "    ax.scatter(param[0], param[1], alpha=0.7, edgecolors='w', s=60)\n",
        "    ax.plot([min_val, max_val], [min_val, max_val], 'k--')  # line X = Y\n",
        "    ax.set_title(title, fontsize=16)\n",
        "    if title != 'Phase Shift':\n",
        "      ax.set_xlabel(f' {title} (from a)', fontsize=10)\n",
        "      ax.set_ylabel(f'{title} (to c)', fontsize=10)\n",
        "    else:\n",
        "      ax.set_xlabel('Phase shift (from a) + Phase shift (from b)', fontsize=10)\n",
        "      ax.set_ylabel('Phase shift (to c)', fontsize=10)\n",
        "\n",
        "# Add overarching title\n",
        "fig.suptitle('Cosine Parameters for Input and Output Weights', fontsize=20)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dcm7xYt8tuT"
      },
      "outputs": [],
      "source": [
        "data_set = np.zeros((p*p, 2*p))\n",
        "labels = np.zeros((p*p, p))\n",
        "for number1 in range(p):\n",
        "  for number2 in range(p):\n",
        "    data_set[number1*p + number2][number1] = 1\n",
        "    data_set[number1*p + number2][number2+p] = 1\n",
        "    labels[number1*p + number2][(number1 + number2) % p] = 1\n",
        "\n",
        "data = torch.from_numpy(data_set).float().to(device)\n",
        "labels = torch.from_numpy(labels).float().to(device)\n",
        "\n",
        "\n",
        "performances = []\n",
        "losses = []\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "for i in range(100):\n",
        "    output = model.forward_with_replaced_weights(data, 0, plot=False, altered_i=[i])\n",
        "    val_preds = torch.argmax(output, axis=1)\n",
        "    correct_val_preds = (val_preds == torch.argmax(labels, axis=1)).sum().item()\n",
        "    total_val_preds = len(val_preds)\n",
        "    performances.append(correct_val_preds/total_val_preds)\n",
        "    losses.append(loss_fn(output, labels).item())\n",
        "\n",
        "indexes = np.argsort(losses)\n",
        "altered_i = []\n",
        "performances = []\n",
        "losses = []\n",
        "for i in indexes:\n",
        "    altered_i.append(i)\n",
        "    output = model.forward_with_replaced_weights(data, 0, plot=False, altered_i=altered_i)\n",
        "    val_preds = torch.argmax(output, axis=1)\n",
        "    correct_val_preds = (val_preds == torch.argmax(labels, axis=1)).sum().item()\n",
        "    total_val_preds = len(val_preds)\n",
        "    performances.append(correct_val_preds/total_val_preds)\n",
        "    losses.append(loss_fn(output, labels).item())\n",
        "\n",
        "\n",
        "plt.plot(np.arange(100), performances)\n",
        "plt.title(\"Replacing Neurons weights with their Cosine Approximation\")\n",
        "plt.xlabel(\"Number of neurons replaced\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YafgF7zYu_BB"
      },
      "outputs": [],
      "source": [
        "accuracies = []\n",
        "n_cos = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
        "for number_of_cosines in n_cos:\n",
        "\n",
        "  sampled_s1s = np.random.uniform(-np.pi, np.pi, size=number_of_cosines)\n",
        "  sampled_s2s = np.random.uniform(-np.pi, np.pi, size=number_of_cosines)\n",
        "  sampled_ws = ((2*np.pi)/p) * np.random.randint(0, p, size=number_of_cosines)\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for j in range(100):\n",
        "      a = np.random.randint(p)\n",
        "      b = np.random.randint(p)\n",
        "      computed_logits = np.zeros((p))\n",
        "      for i in range(number_of_cosines):\n",
        "        for c in range(p):\n",
        "          computed_logits[c] += np.cos(sampled_ws[i]*c + sampled_s1s[i] + sampled_s2s[i])* \\\n",
        "                                0.5*np.maximum((np.cos(sampled_ws[i]*a + sampled_s1s[i]) + \\\n",
        "                                                np.cos(sampled_ws[i]*b + sampled_s2s[i])), 0)\n",
        "      if np.argmax(computed_logits) == (a+b) % p:\n",
        "        correct += 1\n",
        "      total += 1\n",
        "\n",
        "  accuracies.append(correct / total)\n",
        "\n",
        "\n",
        "plt.plot(n_cos, accuracies)\n",
        "plt.title(\"Accuracy with Random Cosines Neurons\")\n",
        "plt.xlabel(\"Number of Neurons\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id3_XW3wm5bI"
      },
      "source": [
        "# Polar plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoQHJisi0Tv4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a new figure\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "\n",
        "# Create a polar subplots\n",
        "ax_1 = fig.add_subplot(221, polar=True)\n",
        "ax_2 = fig.add_subplot(222, polar=True)\n",
        "ax_3 = fig.add_subplot(223, polar=True)\n",
        "ax_4 = fig.add_subplot(224, polar=True)\n",
        "\n",
        "\n",
        "# Generate the x values\n",
        "x = np.linspace(0, 113, num=1000)\n",
        "\n",
        "# Calculate the cosine values\n",
        "y0 = np.cos(0.5*x + 10)\n",
        "\n",
        "\n",
        "thetas_a = []\n",
        "thetas_b = []\n",
        "thetas_c = []\n",
        "weights_a = []\n",
        "weights_b = []\n",
        "weights_c = []\n",
        "activations = []\n",
        "for a in range(113):\n",
        "  for b in range(113):\n",
        "    c = (a+b) % 113\n",
        "    thetas_a.append(2*np.pi*a / 113)\n",
        "    thetas_b.append(2*np.pi*b / 113)\n",
        "    thetas_c.append(2*np.pi*c / 113)\n",
        "    weights_a.append(0.5*np.cos(0.5*a + 10))\n",
        "    weights_b.append(0.5*np.cos(0.5*b + 5))\n",
        "    weights_c.append(0.8*np.cos(0.5*c + 15))\n",
        "    activations.append(np.maximum(0.5*np.cos(0.5*a + 10)  + 0.5*np.cos(0.5*b + 5), 0))\n",
        "\n",
        "ax_1.plot(thetas_a, weights_a, c='red')\n",
        "ax_2.plot(thetas_b, weights_b, c='red')\n",
        "ax_3.scatter(thetas_c, activations, c='red', alpha=0.5, s=1)\n",
        "ax_4.plot(thetas_c, weights_c, c='red')\n",
        "\n",
        "\n",
        "for ax in [ax_1, ax_2, ax_3, ax_4]:\n",
        "  # Plot y=0 circle\n",
        "  theta_zero = np.linspace(0, 2*np.pi, 100)\n",
        "  r_zero = np.zeros_like(theta_zero)\n",
        "  ax.plot(theta_zero, r_zero, 'k--', label=\"y=0\")\n",
        "\n",
        "  # Plot y=-1.5 circle\n",
        "  r_negative_1_5 = -1.5 * np.ones_like(theta_zero)\n",
        "  ax.plot(theta_zero, r_negative_1_5, 'r--', label=\"y=-1.5\")\n",
        "\n",
        "  # Plot y=-1.0 circle\n",
        "  r_negative_1_5 = -1 * np.ones_like(theta_zero)\n",
        "  ax.plot(theta_zero, r_negative_1_5, 'r--', label=\"y=-1.5\", alpha=0)\n",
        "\n",
        "  # Plot y=1.0 circle\n",
        "  r_negative_1_5 = 1 * np.ones_like(theta_zero)\n",
        "  ax.plot(theta_zero, r_negative_1_5, 'r--', label=\"y=-1.5\", alpha=0)\n",
        "\n",
        "  # Generate ticks and labels\n",
        "  ticks = np.linspace(0, 2*np.pi, num=113, endpoint=False) # Adjusted endpoint to False\n",
        "  labels = list(range(113)) # Adjusted range to 113\n",
        "\n",
        "  # Turn off the default ticks and labels\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "\n",
        "  # Add new labels\n",
        "  r_label = ax.get_rmax() + 0.10 # Position for labels\n",
        "  for i in range(len(ticks)):\n",
        "      # Compute the angle to rotate text\n",
        "      rotation = np.rad2deg(ticks[i])\n",
        "\n",
        "      # Add the text to the plot with center alignment\n",
        "      ax.text(ticks[i], r_label, str(labels[i]), rotation=rotation,\n",
        "              rotation_mode='anchor', ha='center')\n",
        "\n",
        "  # Set the radial limits to include 0 and -1.5\n",
        "  y_min = -1.5\n",
        "  y_max = 1.5\n",
        "  ax.set_ylim([y_min, y_max])\n",
        "\n",
        "  # Hide the spines (frame of the plot)\n",
        "  ax.spines['polar'].set_visible(False)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FxjQkeJ41roW"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
