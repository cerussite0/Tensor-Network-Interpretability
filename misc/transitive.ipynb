{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "245297fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "72d8048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 59\n",
    "dataset = np.zeros((2*N, N+1))\n",
    "\n",
    "for i in range(N):\n",
    "    dataset[i][N]=0\n",
    "    dataset[i][i]=1\n",
    "        \n",
    "for i in range(N, 2*N):\n",
    "    dataset[i][N]=1\n",
    "    dataset[i][i-N]=1\n",
    "            \n",
    "labels = np.zeros((2*N, N))\n",
    "for i in range(N):\n",
    "    one_idx = (i+10)%N\n",
    "    labels[i][one_idx]=1\n",
    "\n",
    "for i in range(N, 2*N):\n",
    "    one_idx = (i-N+20)%N\n",
    "    labels[i][one_idx]=1\n",
    "            \n",
    "# first half of the dataset is one relation-0 and the second half on relation-1\n",
    "# last number in input represents the relation number which is then convert to a N-dim vector by the model, so that finally we have e_h and e_r of same dims\n",
    "\n",
    "# First half (clean relation-0 samples)\n",
    "first_half_data = dataset[:N]\n",
    "first_half_labels = labels[:N]\n",
    "\n",
    "# Second half (relation-1 samples)\n",
    "second_half_data = dataset[N:]\n",
    "second_half_labels = labels[N:]\n",
    "\n",
    "# Validation size from second half\n",
    "val_size_second_half = 10  # pick whatever small number you want\n",
    "\n",
    "# Shuffle the second half only\n",
    "perm = np.random.permutation(N)\n",
    "second_half_data = second_half_data[perm]\n",
    "second_half_labels = second_half_labels[perm]\n",
    "\n",
    "# Split second half\n",
    "val_data = second_half_data[:val_size_second_half]\n",
    "val_labels = second_half_labels[:val_size_second_half]\n",
    "\n",
    "train_data = np.concatenate([\n",
    "    first_half_data,              # ALL first half goes to train\n",
    "    second_half_data[val_size_second_half:]   # the rest of second half\n",
    "], axis=0)\n",
    "\n",
    "train_labels = np.concatenate([\n",
    "    first_half_labels,\n",
    "    second_half_labels[val_size_second_half:]\n",
    "], axis=0)\n",
    "\n",
    "# (Optional) Shuffle train set only\n",
    "perm_train = np.random.permutation(train_data.shape[0])\n",
    "train_data = train_data[perm_train]\n",
    "train_labels = train_labels[perm_train]\n",
    "\n",
    "# Convert to tensors\n",
    "train_data = torch.from_numpy(train_data).float()\n",
    "train_labels = torch.from_numpy(train_labels).float()\n",
    "val_data = torch.from_numpy(val_data).float()\n",
    "val_labels = torch.from_numpy(val_labels).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "56467c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilinearMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.entity_embed = nn.Embedding(N, input_size)  # NEW: embeddings for numbers 0..N-1\n",
    "        self.relation_embed = nn.Embedding(2, input_size)  # same as your old embed\n",
    "        \n",
    "        self.bl = nn.Bilinear(input_size, input_size, hidden_size, bias=False)\n",
    "        self.lin = nn.Linear(hidden_size, output_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x[:, :N] is 1-hot for entity\n",
    "        entity_idx = torch.argmax(x[:, :N], dim=1).long()\n",
    "        rel_idx = x[:, N].long()\n",
    "\n",
    "        e_h = self.entity_embed(entity_idx)       # (batch, input_size)\n",
    "        e_r = self.relation_embed(rel_idx)        # (batch, input_size)\n",
    "\n",
    "        h = self.bl(e_h, e_r)\n",
    "        logits = self.lin(h)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e65a4a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_scaled_default_init(model: nn.Module, scale: float = 5.0):\n",
    "#     \"\"\"\n",
    "#     Apply PyTorch defaults (Kaiming/Xavier where appropriate) then multiply weights by `scale`.\n",
    "#     This preserves relative structure but makes them larger.\n",
    "#     \"\"\"\n",
    "#     def _init(m):\n",
    "#         if isinstance(m, nn.Embedding):\n",
    "#             nn.init.normal_(m.weight, mean=0.0, std=1.0)\n",
    "#             m.weight.data.mul_(scale)\n",
    "#         elif isinstance(m, nn.Bilinear):\n",
    "#             # use kaiming for bilinear weights flattened -> still ok to use normal then scale\n",
    "#             nn.init.kaiming_normal_(m.weight.view(m.weight.size(0), -1), a=0, mode='fan_in', nonlinearity='linear')\n",
    "#             m.weight.data.mul_(scale)\n",
    "#             if m.bias is not None:\n",
    "#                 nn.init.zeros_(m.bias)\n",
    "#         elif isinstance(m, nn.Linear):\n",
    "#             nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in', nonlinearity='linear')\n",
    "#             m.weight.data.mul_(scale)\n",
    "#             if m.bias is not None:\n",
    "#                 nn.init.zeros_(m.bias)\n",
    "#     model.apply(_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a4637743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, train_labels, val_data, val_labels, epochs=100, batch_size=32, lr=0.003):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_loss_values = []\n",
    "    val_loss_values = []\n",
    "    train_acc_values = []\n",
    "    val_acc_values = []\n",
    "    \n",
    "    running_train_loss = 0\n",
    "    print(epochs)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        correct_train_preds = 0\n",
    "        total_train_preds = 0\n",
    "        for batch in range(0, len(train_data), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_data[batch:batch+batch_size])\n",
    "            loss = loss_fn(output, torch.argmax(train_labels[batch:batch+batch_size], axis=1)) \n",
    "            running_train_loss += loss.item()\n",
    "            preds = torch.argmax(output, axis=1)\n",
    "            correct_train_preds += (preds == torch.argmax(train_labels[batch:batch+batch_size], axis=1)).sum().item()\n",
    "            total_train_preds += len(preds)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "\n",
    "\n",
    "        output = model(val_data)\n",
    "        val_loss = loss_fn(output, torch.argmax(val_labels, axis=1)).item()\n",
    "        val_preds = torch.argmax(output, axis=1)\n",
    "        correct_val_preds = (val_preds == torch.argmax(val_labels, axis=1)).sum().item()\n",
    "        total_val_preds = len(val_preds)\n",
    "        avg_train_loss = running_train_loss / (len(train_data) / batch_size)\n",
    "        train_acc = correct_train_preds / total_train_preds\n",
    "        val_acc = correct_val_preds / total_val_preds\n",
    "        train_loss_values.append(avg_train_loss)\n",
    "        val_loss_values.append(val_loss)\n",
    "        train_acc_values.append(train_acc)\n",
    "        val_acc_values.append(val_acc)\n",
    "\n",
    "        print(\"Epoch: {} | Train loss: {:.2f} | Validation loss: {:.2f} | Train accuracy: {:.2f} | Validation accuracy: {:.2f}\".format(epoch, avg_train_loss, val_loss, train_acc, val_acc))\n",
    "\n",
    "        running_train_loss = 0\n",
    "    return model, train_loss_values, val_loss_values, train_acc_values, val_acc_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2b1ee120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Epoch: 0 | Train loss: 6.92 | Validation loss: 9.02 | Train accuracy: 0.03 | Validation accuracy: 0.00\n",
      "Epoch: 1 | Train loss: 0.06 | Validation loss: 10.24 | Train accuracy: 0.99 | Validation accuracy: 0.00\n",
      "Epoch: 2 | Train loss: 0.00 | Validation loss: 10.66 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 3 | Train loss: 0.00 | Validation loss: 10.58 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 4 | Train loss: 0.00 | Validation loss: 10.25 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 5 | Train loss: 0.00 | Validation loss: 9.80 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 6 | Train loss: 0.00 | Validation loss: 9.29 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 7 | Train loss: 0.00 | Validation loss: 8.77 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 8 | Train loss: 0.00 | Validation loss: 8.27 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 9 | Train loss: 0.00 | Validation loss: 7.78 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 10 | Train loss: 0.01 | Validation loss: 7.33 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 11 | Train loss: 0.01 | Validation loss: 6.92 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 12 | Train loss: 0.01 | Validation loss: 6.54 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 13 | Train loss: 0.02 | Validation loss: 6.21 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 14 | Train loss: 0.02 | Validation loss: 5.91 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 15 | Train loss: 0.03 | Validation loss: 5.65 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 16 | Train loss: 0.03 | Validation loss: 5.44 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 17 | Train loss: 0.03 | Validation loss: 5.26 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 18 | Train loss: 0.03 | Validation loss: 5.12 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 19 | Train loss: 0.04 | Validation loss: 5.01 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 20 | Train loss: 0.04 | Validation loss: 4.92 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 21 | Train loss: 0.04 | Validation loss: 4.85 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 22 | Train loss: 0.04 | Validation loss: 4.80 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 23 | Train loss: 0.04 | Validation loss: 4.75 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 24 | Train loss: 0.04 | Validation loss: 4.72 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 25 | Train loss: 0.04 | Validation loss: 4.69 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 26 | Train loss: 0.04 | Validation loss: 4.66 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 27 | Train loss: 0.04 | Validation loss: 4.64 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 28 | Train loss: 0.04 | Validation loss: 4.62 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 29 | Train loss: 0.04 | Validation loss: 4.61 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 30 | Train loss: 0.05 | Validation loss: 4.59 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 31 | Train loss: 0.05 | Validation loss: 4.58 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 32 | Train loss: 0.05 | Validation loss: 4.56 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 33 | Train loss: 0.05 | Validation loss: 4.55 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 34 | Train loss: 0.05 | Validation loss: 4.54 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 35 | Train loss: 0.05 | Validation loss: 4.54 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 36 | Train loss: 0.05 | Validation loss: 4.53 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 37 | Train loss: 0.05 | Validation loss: 4.52 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 38 | Train loss: 0.05 | Validation loss: 4.52 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 39 | Train loss: 0.05 | Validation loss: 4.51 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 40 | Train loss: 0.05 | Validation loss: 4.51 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 41 | Train loss: 0.05 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 42 | Train loss: 0.06 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 43 | Train loss: 0.06 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 44 | Train loss: 0.06 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 45 | Train loss: 0.06 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 46 | Train loss: 0.06 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 47 | Train loss: 0.06 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 48 | Train loss: 0.06 | Validation loss: 4.48 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 49 | Train loss: 0.06 | Validation loss: 4.48 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 50 | Train loss: 0.06 | Validation loss: 4.48 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 51 | Train loss: 0.06 | Validation loss: 4.48 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 52 | Train loss: 0.06 | Validation loss: 4.48 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 53 | Train loss: 0.07 | Validation loss: 4.48 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 54 | Train loss: 0.07 | Validation loss: 4.48 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 55 | Train loss: 0.07 | Validation loss: 4.48 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 56 | Train loss: 0.07 | Validation loss: 4.48 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 57 | Train loss: 0.07 | Validation loss: 4.48 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 58 | Train loss: 0.07 | Validation loss: 4.48 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 59 | Train loss: 0.07 | Validation loss: 4.48 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 60 | Train loss: 0.07 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 61 | Train loss: 0.07 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 62 | Train loss: 0.07 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 63 | Train loss: 0.07 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 64 | Train loss: 0.08 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 65 | Train loss: 0.08 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 66 | Train loss: 0.08 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 67 | Train loss: 0.08 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 68 | Train loss: 0.08 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 69 | Train loss: 0.08 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 70 | Train loss: 0.08 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 71 | Train loss: 0.08 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 72 | Train loss: 0.08 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 73 | Train loss: 0.08 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 74 | Train loss: 0.08 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 75 | Train loss: 0.08 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 76 | Train loss: 0.09 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 77 | Train loss: 0.09 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 78 | Train loss: 0.09 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 79 | Train loss: 0.09 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 80 | Train loss: 0.09 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 81 | Train loss: 0.09 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 82 | Train loss: 0.09 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 83 | Train loss: 0.09 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 84 | Train loss: 0.09 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 85 | Train loss: 0.09 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 86 | Train loss: 0.09 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 87 | Train loss: 0.09 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 88 | Train loss: 0.09 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 89 | Train loss: 0.09 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 90 | Train loss: 0.10 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 91 | Train loss: 0.10 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 92 | Train loss: 0.10 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 93 | Train loss: 0.10 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 94 | Train loss: 0.10 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 95 | Train loss: 0.10 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 96 | Train loss: 0.10 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 97 | Train loss: 0.10 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 98 | Train loss: 0.10 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 99 | Train loss: 0.10 | Validation loss: 4.50 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 100 | Train loss: 0.10 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 101 | Train loss: 0.10 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 102 | Train loss: 0.10 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 103 | Train loss: 0.10 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 104 | Train loss: 0.10 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 105 | Train loss: 0.10 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 106 | Train loss: 0.10 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 107 | Train loss: 0.10 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 108 | Train loss: 0.10 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 109 | Train loss: 0.10 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 110 | Train loss: 0.10 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 111 | Train loss: 0.10 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 112 | Train loss: 0.11 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 113 | Train loss: 0.11 | Validation loss: 4.49 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 114 | Train loss: 0.11 | Validation loss: 4.48 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 115 | Train loss: 0.11 | Validation loss: 4.48 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 116 | Train loss: 0.11 | Validation loss: 4.48 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 117 | Train loss: 0.11 | Validation loss: 4.48 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 118 | Train loss: 0.11 | Validation loss: 4.48 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 119 | Train loss: 0.11 | Validation loss: 4.48 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 120 | Train loss: 0.11 | Validation loss: 4.48 | Train accuracy: 1.00 | Validation accuracy: 0.00\n",
      "Epoch: 121 | Train loss: 0.11 | Validation loss: 4.48 | Train accuracy: 1.00 | Validation accuracy: 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m BilinearMLP(N, \u001b[38;5;241m100\u001b[39m, N)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# apply_scaled_default_init(model, scale=20.0)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model, train_loss_values, val_loss_values, train_acc_values, val_acc_values \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.003\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[74], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_data, train_labels, val_data, val_labels, epochs, batch_size, lr)\u001b[0m\n\u001b[0;32m     22\u001b[0m     correct_train_preds \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(train_labels[batch:batch\u001b[38;5;241m+\u001b[39mbatch_size], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     23\u001b[0m     total_train_preds \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(preds)\n\u001b[1;32m---> 24\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\Aditya\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Aditya\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Aditya\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = BilinearMLP(N, 100, N)\n",
    "# apply_scaled_default_init(model, scale=20.0)\n",
    "model, train_loss_values, val_loss_values, train_acc_values, val_acc_values = train(model, train_data, train_labels, val_data, val_labels, epochs=1000, batch_size=16, lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a976e6dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
